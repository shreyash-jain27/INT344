{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸŒ English to Punjabi Translation Model Training\n",
                "\n",
                "This notebook contains the complete pipeline for fine-tuning a transformer model for English to Punjabi translation. \n",
                "\n",
                "**Project:** NLP Project - Annual Report Summarizer  \n",
                "**Task:** Multilingual Integration (Punjabi)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!pip install transformers[torch] datasets sacrebleu sentencepiece evaluate rouge_score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "try:\n",
                "    print(\"ğŸ” Loading OPUS-100 for English-Punjabi...\")\n",
                "    raw_datasets = load_dataset(\"opus100\", \"en-pa\")\n",
                "    print(\"âœ… Successfully loaded OPUS-100!\")\n",
                "except Exception as e:\n",
                "    print(f\"âŒ Failed to load dataset: {e}\")\n",
                "    raise RuntimeError(\"Could not load any English-Punjabi dataset.\")\n",
                "\n",
                "print(raw_datasets)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Initialize Model and Tokenizer\n",
                "For Indian languages, Helsinki-NLP uses a group model called `Helsinki-NLP/opus-mt-en-inc` (inc = Indic). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
                "\n",
                "model_checkpoint = \"Helsinki-NLP/opus-mt-en-inc\"\n",
                "\n",
                "print(f\"ğŸ¤– Loading model and tokenizer: {model_checkpoint}\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
                "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
                "print(\"âœ… Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_input_length = 128\n",
                "max_target_length = 128\n",
                "target_token = \">>pan<< \" \n",
                "\n",
                "def preprocess_function(examples):\n",
                "    if \"translation\" in examples:\n",
                "        inputs = [target_token + ex.get(\"en\", \"\") for ex in examples[\"translation\"]]\n",
                "        targets = [ex.get(\"pa\", \"\") for ex in examples[\"translation\"]]\n",
                "    else:\n",
                "        inputs = [target_token + text for text in examples.get(\"en\", [])]\n",
                "        targets = examples.get(\"pa\", [])\n",
                "        \n",
                "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
                "\n",
                "    with tokenizer.as_target_tokenizer():\n",
                "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
                "\n",
                "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
                "    return model_inputs\n",
                "\n",
                "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Evaluation Metrics\n",
                "To measure how good the model is, we use **BLEU** and **chrF++**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import evaluate\n",
                "import numpy as np\n",
                "\n",
                "metric = evaluate.load(\"sacrebleu\")\n",
                "chrf_metric = evaluate.load(\"chrf\")\n",
                "\n",
                "def postprocess_text(preds, labels):\n",
                "    preds = [pred.strip() for pred in preds]\n",
                "    labels = [[label.strip()] for label in labels]\n",
                "    return preds, labels\n",
                "\n",
                "def compute_metrics(eval_preds):\n",
                "    preds, labels = eval_preds\n",
                "    if isinstance(preds, tuple):\n",
                "        preds = preds[0]\n",
                "    \n",
                "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
                "\n",
                "    # Replace -100 in the labels since we can't decode them\n",
                "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
                "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
                "\n",
                "    # Some simple post-processing\n",
                "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
                "\n",
                "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
                "    chrf = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
                "    \n",
                "    return {\n",
                "        \"bleu\": result[\"score\"], \n",
                "        \"chrf\": chrf[\"score\"],\n",
                "        \"gen_len\": np.mean([np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds])\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Training Setup (SKIP IF ALREADY TRAINED)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
                "import torch\n",
                "\n",
                "train_sample_size = 30000 \n",
                "train_dataset = tokenized_datasets[\"train\"].select(range(min(train_sample_size, len(tokenized_datasets[\"train\"]))))\n",
                "\n",
                "batch_size = 16\n",
                "args = Seq2SeqTrainingArguments(\n",
                "    \"punjabi-translator-finetuned\",\n",
                "    evaluation_strategy = \"epoch\",\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=batch_size,\n",
                "    per_device_eval_batch_size=batch_size,\n",
                "    weight_decay=0.01,\n",
                "    save_total_limit=3,\n",
                "    num_train_epochs=3,\n",
                "    predict_with_generate=True,\n",
                "    fp16=True if torch.cuda.is_available() else False,\n",
                "    push_to_hub=False\n",
                ")\n",
                "\n",
                "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
                "\n",
                "trainer = Seq2SeqTrainer(\n",
                "    model,\n",
                "    args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=tokenized_datasets[\"validation\"],\n",
                "    data_collator=data_collator,\n",
                "    tokenizer=tokenizer,\n",
                "    compute_metrics=compute_metrics\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7. Fine-tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# trainer.train() # Uncomment to train"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8. Testing the Saved Model\n",
                "Run this section if you want to test your already saved model from the `./models/punjabi_translator` folder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
                "import torch\n",
                "import pandas as pd\n",
                "\n",
                "model_path = \"./models/punjabi_translator\"\n",
                "\n",
                "print(f\"ğŸš€ Loading saved model from {model_path}...\")\n",
                "test_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
                "test_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    test_model = test_model.to(\"cuda\")\n",
                "\n",
                "# Re-setup trainer for evaluation only\n",
                "test_args = Seq2SeqTrainingArguments(\n",
                "    \"eval_output\",\n",
                "    predict_with_generate=True,\n",
                "    per_device_eval_batch_size=16,\n",
                "    fp16=True if torch.cuda.is_available() else False\n",
                ")\n",
                "\n",
                "test_trainer = Seq2SeqTrainer(\n",
                "    test_model,\n",
                "    test_args,\n",
                "    data_collator=data_collator,\n",
                "    tokenizer=test_tokenizer,\n",
                "    compute_metrics=compute_metrics\n",
                ")\n",
                "\n",
                "print(\"ğŸ“Š Running evaluation on full test set...\")\n",
                "test_metrics = test_trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"], metric_key_prefix=\"test\")\n",
                "\n",
                "print(f\"\\nğŸ† Final Test BLEU Score: {test_metrics.get('test_bleu', 0):.2f}\")\n",
                "print(f\"ğŸ† Final Test chrF++ Score: {test_metrics.get('test_chrf', 0):.2f}\")\n",
                "\n",
                "# Sample translations\n",
                "print(\"\\nğŸ‘€ Generating qualitative sample results...\")\n",
                "test_samples = raw_datasets[\"test\"].select(range(5))\n",
                "qualitative_results = []\n",
                "\n",
                "for sample in test_samples:\n",
                "    en_text = sample[\"translation\"][\"en\"]\n",
                "    pa_ref = sample[\"translation\"][\"pa\"]\n",
                "    input_text = target_token + en_text\n",
                "    \n",
                "    inputs = test_tokenizer(input_text, return_tensors=\"pt\").to(test_model.device)\n",
                "    with torch.no_grad():\n",
                "        outputs = test_model.generate(**inputs, max_length=128, num_beams=5)\n",
                "    pa_pred = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    \n",
                "    qualitative_results.append({\n",
                "        \"English Source\": en_text,\n",
                "        \"Human Punjabi (Reference)\": pa_ref,\n",
                "        \"Model Punjabi (Output)\": pa_pred\n",
                "    })\n",
                "\n",
                "display(pd.DataFrame(qualitative_results))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9. Interactive Manual Test\n",
                "Type any English sentence below to see how the model translates it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def translate_sentence(sentence):\n",
                "    input_text = target_token + sentence\n",
                "    inputs = test_tokenizer(input_text, return_tensors=\"pt\").to(test_model.device)\n",
                "    with torch.no_grad():\n",
                "        outputs = test_model.generate(**inputs, max_length=128, num_beams=5)\n",
                "    return test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "my_sentence = \"The company has achieved significant growth in the rural areas.\"\n",
                "print(f\"English: {my_sentence}\")\n",
                "print(f\"Punjabi: {translate_sentence(my_sentence)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}