{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåç English to Punjabi Translation Model Training\n",
                "\n",
                "This notebook contains the complete pipeline for fine-tuning a transformer model for English to Punjabi translation. \n",
                "\n",
                "**Project:** NLP Project - Annual Report Summarizer  \n",
                "**Task:** Multilingual Integration (Punjabi)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#!pip install transformers[torch] datasets sacrebleu sentencepiece evaluate rouge_score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Loading OPUS-100 for English-Punjabi...\n",
                        "‚úÖ Successfully loaded OPUS-100!\n",
                        "DatasetDict({\n",
                        "    test: Dataset({\n",
                        "        features: ['translation'],\n",
                        "        num_rows: 2000\n",
                        "    })\n",
                        "    train: Dataset({\n",
                        "        features: ['translation'],\n",
                        "        num_rows: 107296\n",
                        "    })\n",
                        "    validation: Dataset({\n",
                        "        features: ['translation'],\n",
                        "        num_rows: 2000\n",
                        "    })\n",
                        "})\n"
                    ]
                }
            ],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "try:\n",
                "    print(\"üîç Loading OPUS-100 for English-Punjabi...\")\n",
                "    raw_datasets = load_dataset(\"opus100\", \"en-pa\")\n",
                "    print(\"‚úÖ Successfully loaded OPUS-100!\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Failed to load dataset: {e}\")\n",
                "    raise RuntimeError(\"Could not load any English-Punjabi dataset.\")\n",
                "\n",
                "print(raw_datasets)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Initialize Model and Tokenizer\n",
                "For Indian languages, Helsinki-NLP uses a group model called `Helsinki-NLP/opus-mt-en-inc` (inc = Indic). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ü§ñ Loading model and tokenizer: Helsinki-NLP/opus-mt-en-inc\n",
                        "‚úÖ Model loaded successfully!\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
                "\n",
                "model_checkpoint = \"Helsinki-NLP/opus-mt-en-inc\"\n",
                "\n",
                "print(f\"ü§ñ Loading model and tokenizer: {model_checkpoint}\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
                "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
                "print(\"‚úÖ Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7c0c80ef0280472389ec110447e31622",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\2005s\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "50de00b9a7844372b7628bd4d08a3b54",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/107296 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e55ee55abf1d4004a790f62a1aa1700e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "max_input_length = 128\n",
                "max_target_length = 128\n",
                "target_token = \">>pan<< \" \n",
                "\n",
                "def preprocess_function(examples):\n",
                "    if \"translation\" in examples:\n",
                "        inputs = [target_token + ex.get(\"en\", \"\") for ex in examples[\"translation\"]]\n",
                "        targets = [ex.get(\"pa\", \"\") for ex in examples[\"translation\"]]\n",
                "    else:\n",
                "        inputs = [target_token + text for text in examples.get(\"en\", [])]\n",
                "        targets = examples.get(\"pa\", [])\n",
                "        \n",
                "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
                "\n",
                "    with tokenizer.as_target_tokenizer():\n",
                "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
                "\n",
                "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
                "    return model_inputs\n",
                "\n",
                "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Evaluation Metrics\n",
                "To measure how good the model is, we use **BLEU** and **chrF++**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4a224ff669e641928f0d7a6471674e1b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading builder script: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e896df7fc95641ce8cc8a884e33a059e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading builder script: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "import evaluate\n",
                "import numpy as np\n",
                "\n",
                "metric = evaluate.load(\"sacrebleu\")\n",
                "chrf_metric = evaluate.load(\"chrf\")\n",
                "\n",
                "def postprocess_text(preds, labels):\n",
                "    preds = [pred.strip() for pred in preds]\n",
                "    labels = [[label.strip()] for label in labels]\n",
                "    return preds, labels\n",
                "\n",
                "def compute_metrics(eval_preds):\n",
                "    preds, labels = eval_preds\n",
                "    if isinstance(preds, tuple):\n",
                "        preds = preds[0]\n",
                "    \n",
                "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
                "\n",
                "    # Replace -100 in the labels since we can't decode them\n",
                "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
                "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
                "\n",
                "    # Some simple post-processing\n",
                "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
                "\n",
                "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
                "    chrf = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
                "    \n",
                "    return {\n",
                "        \"bleu\": result[\"score\"], \n",
                "        \"chrf\": chrf[\"score\"],\n",
                "        \"gen_len\": np.mean([np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds])\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Training Setup (SKIP IF ALREADY TRAINED)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\2005s\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
                        "  warnings.warn(\n",
                        "C:\\Users\\2005s\\AppData\\Local\\Temp\\ipykernel_15480\\2686659652.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
                        "  trainer = Seq2SeqTrainer(\n"
                    ]
                }
            ],
            "source": [
                "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
                "import torch\n",
                "\n",
                "train_sample_size = 30000 \n",
                "train_dataset = tokenized_datasets[\"train\"].select(range(min(train_sample_size, len(tokenized_datasets[\"train\"]))))\n",
                "\n",
                "batch_size = 16\n",
                "args = Seq2SeqTrainingArguments(\n",
                "    \"punjabi-translator-finetuned\",\n",
                "    evaluation_strategy = \"epoch\",\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=batch_size,\n",
                "    per_device_eval_batch_size=batch_size,\n",
                "    weight_decay=0.01,\n",
                "    save_total_limit=3,\n",
                "    num_train_epochs=3,\n",
                "    predict_with_generate=True,\n",
                "    fp16=True if torch.cuda.is_available() else False,\n",
                "    push_to_hub=False\n",
                ")\n",
                "\n",
                "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
                "\n",
                "trainer = Seq2SeqTrainer(\n",
                "    model,\n",
                "    args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=tokenized_datasets[\"validation\"],\n",
                "    data_collator=data_collator,\n",
                "    tokenizer=tokenizer,\n",
                "    compute_metrics=compute_metrics\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7. Fine-tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# trainer.train() # Uncomment to train"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8. Testing the Saved Model\n",
                "Run this section if you want to test your already saved model from the `./models/punjabi_translator` folder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Loading saved model from ./models/punjabi_translator...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\2005s\\AppData\\Local\\Temp\\ipykernel_15480\\1526053772.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
                        "  test_trainer = Seq2SeqTrainer(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìä Running evaluation on full test set...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [125/125 02:47]\n",
                            "    </div>\n",
                            "    "
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "üèÜ Final Test BLEU Score: 45.01\n",
                        "üèÜ Final Test chrF++ Score: 70.28\n",
                        "\n",
                        "üëÄ Generating qualitative sample results...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>English Source</th>\n",
                            "      <th>Human Punjabi (Reference)</th>\n",
                            "      <th>Model Punjabi (Output)</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>Published</td>\n",
                            "      <td>‡®™‡®¨‡®≤‡®ø‡®∏‡®º ‡®ï‡©Ä‡®§‡©á</td>\n",
                            "      <td>‡®™‡®¨‡®≤‡®ø‡®∏‡®º ‡®ï‡©Ä‡®§‡©á</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>Name:</td>\n",
                            "      <td>‡®®‡®æ‡®Ç:</td>\n",
                            "      <td>‡®®‡®æ‡®Ç:</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>Ignored</td>\n",
                            "      <td>‡®∏‡®∞‡©ã‡®§ ‡®´‡®æ‡®á‡®≥‡®æ‡®Ç:</td>\n",
                            "      <td>‡®∏‡®∞‡©ã‡®§ ‡®´‡®æ‡®á‡®≤‡®º‡®æ‡®Ç:</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>Thank you for using KDE</td>\n",
                            "      <td>KDE ‡®µ‡®∞‡®§‡®£ ‡®≤‡®à ‡®ß‡©∞‡®®‡®µ‡®æ‡®¶</td>\n",
                            "      <td>KDE ‡®µ‡®∞‡®§‡®£ ‡®≤‡®à ‡®ß‡©∞‡®®‡®µ‡®æ‡®¶</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>&amp; Delete</td>\n",
                            "      <td>‡®π‡®ü‡®æ‡®ì( D)</td>\n",
                            "      <td>‡®π‡®ü‡®æ‡®ì( D)</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "            English Source Human Punjabi (Reference) Model Punjabi (Output)\n",
                            "0                Published               ‡®™‡®¨‡®≤‡®ø‡®∏‡®º ‡®ï‡©Ä‡®§‡©á            ‡®™‡®¨‡®≤‡®ø‡®∏‡®º ‡®ï‡©Ä‡®§‡©á\n",
                            "1                    Name:                      ‡®®‡®æ‡®Ç:                   ‡®®‡®æ‡®Ç:\n",
                            "2                  Ignored              ‡®∏‡®∞‡©ã‡®§ ‡®´‡®æ‡®á‡®≥‡®æ‡®Ç:          ‡®∏‡®∞‡©ã‡®§ ‡®´‡®æ‡®á‡®≤‡®º‡®æ‡®Ç:\n",
                            "3  Thank you for using KDE        KDE ‡®µ‡®∞‡®§‡®£ ‡®≤‡®à ‡®ß‡©∞‡®®‡®µ‡®æ‡®¶     KDE ‡®µ‡®∞‡®§‡®£ ‡®≤‡®à ‡®ß‡©∞‡®®‡®µ‡®æ‡®¶\n",
                            "4                 & Delete                  ‡®π‡®ü‡®æ‡®ì( D)               ‡®π‡®ü‡®æ‡®ì( D)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
                "import torch\n",
                "import pandas as pd\n",
                "\n",
                "model_path = \"./models/punjabi_translator\"\n",
                "\n",
                "print(f\"üöÄ Loading saved model from {model_path}...\")\n",
                "test_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
                "test_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    test_model = test_model.to(\"cuda\")\n",
                "\n",
                "# Re-setup trainer for evaluation only\n",
                "test_args = Seq2SeqTrainingArguments(\n",
                "    \"eval_output\",\n",
                "    predict_with_generate=True,\n",
                "    per_device_eval_batch_size=16,\n",
                "    fp16=True if torch.cuda.is_available() else False\n",
                ")\n",
                "\n",
                "test_trainer = Seq2SeqTrainer(\n",
                "    test_model,\n",
                "    test_args,\n",
                "    data_collator=data_collator,\n",
                "    tokenizer=test_tokenizer,\n",
                "    compute_metrics=compute_metrics\n",
                ")\n",
                "\n",
                "print(\"üìä Running evaluation on full test set...\")\n",
                "test_metrics = test_trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"], metric_key_prefix=\"test\")\n",
                "\n",
                "print(f\"\\nüèÜ Final Test BLEU Score: {test_metrics.get('test_bleu', 0):.2f}\")\n",
                "print(f\"üèÜ Final Test chrF++ Score: {test_metrics.get('test_chrf', 0):.2f}\")\n",
                "\n",
                "# Sample translations\n",
                "print(\"\\nüëÄ Generating qualitative sample results...\")\n",
                "test_samples = raw_datasets[\"test\"].select(range(5))\n",
                "qualitative_results = []\n",
                "\n",
                "for sample in test_samples:\n",
                "    en_text = sample[\"translation\"][\"en\"]\n",
                "    pa_ref = sample[\"translation\"][\"pa\"]\n",
                "    input_text = target_token + en_text\n",
                "    \n",
                "    inputs = test_tokenizer(input_text, return_tensors=\"pt\").to(test_model.device)\n",
                "    with torch.no_grad():\n",
                "        outputs = test_model.generate(**inputs, max_length=128, num_beams=5)\n",
                "    pa_pred = test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    \n",
                "    qualitative_results.append({\n",
                "        \"English Source\": en_text,\n",
                "        \"Human Punjabi (Reference)\": pa_ref,\n",
                "        \"Model Punjabi (Output)\": pa_pred\n",
                "    })\n",
                "\n",
                "display(pd.DataFrame(qualitative_results))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9. Interactive Manual Test\n",
                "Type any English sentence below to see how the model translates it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "English: Despite the overwhelming challenges posed by the rapidly changing climate and the increasing scarcity of natural resources, governments around the world are still struggling to implement effective policies that balance economic growth with environmental sustainability, which has led to debates on the need for more urgent and innovative solutions.\n",
                        "Punjabi: ‡®Æ‡©å‡®∏‡®Æ ‡®§‡©á‡®ú‡®º‡©Ä ‡®®‡®æ‡®≤ ‡®¨‡®¶‡®≤‡®¶‡©á ‡®Æ‡©å‡®∏‡®Æ ‡®Ö‡®§‡©á ‡®ï‡©Å‡®¶‡®∞‡®§‡©Ä ‡®µ‡®æ‡®§‡®æ‡®µ‡®∞‡®£ ‡®¶‡©Ä ‡®µ‡®ß ‡®∞‡®π‡©Ä ‡®∏‡®Æ‡©±‡®∏‡®ø‡®Ü ‡®¶‡©á ‡®¨‡®æ‡®µ‡®ú‡©Ç‡®¶, ‡®∏‡©∞‡®∏‡®æ‡®∞ ‡®≠‡®∞ ‡®¶‡©Ä‡®Ü‡®Ç ‡®∏‡®∞‡®ï‡®æ‡®∞‡®æ‡®Ç ‡®π‡®æ‡®≤‡©á ‡®µ‡©Ä ‡®™‡©ç‡®∞‡®≠‡®æ‡®µ‡®∏‡®º‡®æ‡®≤‡©Ä ‡®™‡®æ‡®≤‡®∏‡©Ä‡®Ü‡®Ç ‡®®‡©Ç‡©∞ ‡®≤‡®æ‡®ó‡©Ç ‡®ï‡®∞‡®® ‡®≤‡®à ‡®∏‡©∞‡®ò‡®∞‡®∏‡®º ‡®ï‡®∞ ‡®∞‡®π‡©Ä‡®Ü‡®Ç ‡®π‡®®, ‡®ú‡©ã ‡®µ‡®æ‡®§‡®æ‡®µ‡®∞‡®£ ‡®®‡©Ç‡©∞ ‡®∏‡®•‡®ø‡®∞ ‡®∞‡©±‡®ñ‡®£ ‡®≤‡®à ‡®Ü‡®∞‡®•‡®ø‡®ï ‡®§‡®∞‡©±‡®ï‡©Ä ‡®®‡©Ç‡©∞ ‡®∏‡©∞‡®§‡©Å‡®≤‡®ø‡®§ ‡®∞‡©±‡®ñ‡®¶‡©Ä‡®Ü‡®Ç ‡®π‡®®, ‡®ú‡©ã ‡®ï‡®ø ‡®π‡©ã‡®∞ ‡®ú‡®º‡®∞‡©Ç‡®∞‡©Ä ‡®Ö‡®§‡©á ‡®®‡®æ‡®ú‡®º‡©Å‡®ï ‡®π‡©±‡®≤‡®æ‡®Ç ‡®¶‡©Ä ‡®≤‡©ã‡©ú ‡®â‡©±‡®§‡©á ‡®¨‡®π‡®ø‡®∏ ‡®ï‡®∞ ‡®∞‡®π‡©Ä‡®Ü‡®Ç ‡®π‡®® ‡•§\n"
                    ]
                }
            ],
            "source": [
                "def translate_sentence(sentence):\n",
                "    input_text = target_token + sentence\n",
                "    inputs = test_tokenizer(input_text, return_tensors=\"pt\").to(test_model.device)\n",
                "    with torch.no_grad():\n",
                "        outputs = test_model.generate(**inputs, max_length=128, num_beams=5)\n",
                "    return test_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "my_sentence = \"Despite the overwhelming challenges posed by the rapidly changing climate and the increasing scarcity of natural resources, governments around the world are still struggling to implement effective policies that balance economic growth with environmental sustainability, which has led to debates on the need for more urgent and innovative solutions.\"\n",
                "print(f\"English: {my_sentence}\")\n",
                "print(f\"Punjabi: {translate_sentence(my_sentence)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
